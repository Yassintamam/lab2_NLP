{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace all digits in a string with zeros.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip())\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words))\n",
    "    )\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
    "        - words (strings)\n",
    "        - word indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    assert mode == 'train' or (mode == 'test' and word_to_id and tag_to_id)\n",
    "\n",
    "    if mode=='train':\n",
    "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
    "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
    "\n",
    "    def f(x): return x\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'tags': tags,\n",
    "        })\n",
    "\n",
    "    if mode == 'train':\n",
    "        return data, {'word_to_id':word_to_id, 'id_to_word':id_to_word, 'tag_to_id':tag_to_id, 'id_to_tag':id_to_tag}\n",
    "    else:\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import codecs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"\n",
    "     HMM, Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dic, decode_type):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_words = len(dic['word_to_id'])\n",
    "        self.num_tags = len(dic['tag_to_id'])\n",
    "\n",
    "        self.initial_prob = np.ones([self.num_tags])\n",
    "        self.transition_prob = np.ones([self.num_tags, self.num_tags])\n",
    "        self.emission_prob = np.ones([self.num_tags, self.num_words])\n",
    "        self.decode_type = decode_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        TODO: Train a bigram HMM model using MLE estimates.\n",
    "        Complete the code to compute self,initial_prob, self.transition_prob and self.emission_prob appropriately.\n",
    "\n",
    "        Args:\n",
    "            corpus is a list of dictionaries of the form:\n",
    "            {'str_words': str_words,   ### List of string words\n",
    "            'words': words,            ### List of word IDs\n",
    "            'tags': tags}              ### List of tag IDs\n",
    "            All three lists above have length equal to the sentence length for each instance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute initial_probs.\n",
    "        # initial_prob[x]: the probability of tag x to be assigned to the first token in a sentence.\n",
    "        self.initial_prob = np.zeros([self.num_tags])\n",
    "        print(self.initial_prob)\n",
    "        for sentence in corpus:\n",
    "            # TODO: update self.initial_prob\n",
    "            # (5 points)\n",
    "            # START HERE\n",
    "            tag = sentence['tags']\n",
    "            self.initial_prob[tag[0]] += 1\n",
    "            # END HERE\n",
    "        print(self.initial_prob)\n",
    "\n",
    "        # Normarlize initial_prob to sum to 1\n",
    "        self.initial_prob /= np.sum(self.initial_prob)\n",
    "\n",
    "        # Step 2: Complete the code to compute transition_prob.\n",
    "        # transition_prob[x][y]: the probability of tag y to appear after tag x.\n",
    "        self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
    "        for sentence in corpus:\n",
    "            tag = sentence['tags']\n",
    "            for i in range(1, len(tag)):\n",
    "                # TODO: update self.transition_prob\n",
    "                # (5 points)\n",
    "                # START HERE\n",
    "                 self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
    "            for sentence in corpus:\n",
    "                tag = sentence['tags']\n",
    "                for i in range(1, len(tag)):\n",
    "                    self.transition_prob[tag[i-1]][tag[i]] += 1\n",
    "                # END HERE\n",
    "        print(self.transition_prob)\n",
    "\n",
    "        # Normalize every row of transition_prob to sum to 1.\n",
    "        for p in self.transition_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "\n",
    "        # Step 3: Complete the code to compute emission_prob\n",
    "        # emission_prob[x][y]: the probability of word y to appear given tag x.\n",
    "        # Note that for each sentence s in the corpus, word IDs are in s['words'].\n",
    "        self.emission_prob = np.zeros([self.num_tags, self.num_words])\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence['tags'])):\n",
    "                # TODO: update self.emission_prob\n",
    "                # (5 points)\n",
    "                # START HERE\n",
    "                tag = sentence['tags'][i]\n",
    "                word = sentence['words'][i]\n",
    "                self.emission_prob[tag][word] += 1\n",
    "                # END HERE\n",
    "        print(self.emission_prob)\n",
    "\n",
    "        # For every tag, normalize the emission_prob to sum to 1.\n",
    "        for p in self.emission_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "        return\n",
    "\n",
    "    def greedy_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        Decode a single sentence in Greedy fashion\n",
    "        Return a list of tags.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "\n",
    "        init_scores = [self.initial_prob[t] * self.emission_prob[t][sentence[0]] for t in range(self.num_tags)]\n",
    "        tags.append(np.argmax(init_scores))\n",
    "\n",
    "        for w in sentence[1:]:\n",
    "            scores = [self.transition_prob[tags[-1]][t] * self.emission_prob[t][w] for t in range(self.num_tags)]\n",
    "            tags.append(np.argmax(scores))\n",
    "\n",
    "        assert len(tags) == len(sentence)\n",
    "        return tags\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        TODO: Complete the code to decode a single sentence using the Viterbi algorithm.\n",
    "        Args:\n",
    "             sentence -- a list of ints that represents word IDs.\n",
    "        Output:\n",
    "             tags     -- a list of ints that represents the tags decoded from the input.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "        l = len(sentence)\n",
    "\n",
    "        pi = self.initial_prob\n",
    "        A = self.transition_prob\n",
    "        O = self.emission_prob\n",
    "\n",
    "        # Let M be the state matrix.\n",
    "        # M[i,j]: most probable sequence of tags ending with tag j at the i-th token.\n",
    "        M = np.zeros((l, self.num_tags))\n",
    "        M[:,:] = float('-inf')\n",
    "\n",
    "        # Use B to track the path to reach the most probable sequence.\n",
    "        # B[i,j] is the tag of the (i-1)-th token in the most probable sequence ending with tag j at the i-th token.\n",
    "        B = np.zeros((l, self.num_tags), 'int')\n",
    "\n",
    "        # Compute the probability to assign each tag to the first token.\n",
    "        M[0, :] = pi * O[:, sentence[0]]\n",
    "\n",
    "        # Dynamic programming.\n",
    "        for i in range(1, l):\n",
    "            for j in range(self.num_tags):\n",
    "                # TODO: Compute M[i, j] and B[i, j].\n",
    "                # (10 points)\n",
    "                # START HERE\n",
    "                max_prob = float('-inf')\n",
    "                max_tag = None\n",
    "                for k in range(self.num_tags):\n",
    "                  prob = M[i-1, k] * A[k, j] * O[j, sentence[i]]\n",
    "                  if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_tag = k\n",
    "                M[i, j] = max_prob\n",
    "                B[i, j] = max_tag\n",
    "                # END HERE\n",
    "\n",
    "        # Extract the optimal sequence of tags from B.\n",
    "        # Start from the last position, and iteratively find the tag of each position that results in the most probable tag sequence.\n",
    "        tags.append(np.argmax(M[l-1,:]))\n",
    "        for i in range(l-1, 0, -1):\n",
    "            # TODO: Extract the tag of the (i-1)-th token that results in the most probable sequence of tags.\n",
    "            # (5 points)\n",
    "            # START HERE\n",
    "            tags.append(B[i, tags[-1]])\n",
    "            # END HERE\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"\n",
    "        Tag a sentence using a trained HMM.\n",
    "        \"\"\"\n",
    "        if self.decode_type == 'viterbi':\n",
    "            return self.viterbi_decode(sentence)\n",
    "        elif self.decode_type == 'greedy':\n",
    "            return self.greedy_decode(sentence)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown decoding type\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def tag_corpus(model, test_corpus, output_file, dic):\n",
    "    if output_file:\n",
    "        f_output = codecs.open(output_file, 'w', 'utf-8')\n",
    "    start = time.time()\n",
    "\n",
    "    num_correct = 0.\n",
    "    num_total = 0.\n",
    "    y_pred=[]\n",
    "    y_actual=[]\n",
    "    print('Tagging...')\n",
    "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
    "        tags = model.tag(sentence['words'])\n",
    "        str_tags = [dic['id_to_tag'][t] for t in tags]\n",
    "        y_pred.extend(tags)\n",
    "        y_actual.extend(sentence['tags'])\n",
    "\n",
    "        # Check accuracy.\n",
    "        num_correct += np.sum(np.array(tags) == np.array(sentence['tags']))\n",
    "        num_total += len([w for w in sentence['words']])\n",
    "\n",
    "        if output_file:\n",
    "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, '__', y)\n",
    "                                             for w, y in zip(sentence['str_words'], str_tags)))\n",
    "\n",
    "    print('---- %i lines tagged in %.4fs ----' % (len(test_corpus), time.time() - start))\n",
    "    if output_file:\n",
    "        f_output.close()\n",
    "\n",
    "    print(\"Overall accuracy: %s\\n\" % (num_correct/num_total))\n",
    "    return y_pred,y_actual"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def compute_score(y_pred,y_actual):\n",
    "    A = confusion_matrix(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred,average=None)\n",
    "    print(\"Confusion Matrix:\\n\", A)\n",
    "    print(\"F-1 scores: \", f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def runHiddenMarkovModel(train_corpus,\n",
    "                         test_corpus,\n",
    "                         dic,\n",
    "                         decode_type,\n",
    "                         output_file):\n",
    "    # build and train the model\n",
    "    model = HMM(dic, decode_type)\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    print(\"Train results:\")\n",
    "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic['id_to_tag'])\n",
    "    A = compute_score(pred,real)\n",
    "\n",
    "    # test on validation\n",
    "    print(\"\\n-----------\\nTest results:\")\n",
    "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic['id_to_tag'])\n",
    "    A = compute_score(pred,real)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded successfully and saved at eng.train\n",
      "Dataset downloaded successfully and saved at eng.val\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "import requests\n",
    "\n",
    "def download_dataset(url, save_path):\n",
    "    \"\"\"\n",
    "    Download dataset from URL and save it to the specified path.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Dataset downloaded successfully and saved at {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download dataset from {url}\")\n",
    "\n",
    "# Download the training dataset\n",
    "train_url = \"https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\"\n",
    "train_save_path = \"eng.train\"\n",
    "download_dataset(train_url, train_save_path)\n",
    "\n",
    "# Download the validation dataset\n",
    "val_url = \"https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\"\n",
    "val_save_path = \"eng.val\"\n",
    "download_dataset(val_url, val_save_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20101 unique words (203621 in total)\n",
      "Found 5 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "train_file = 'eng.train'\n",
    "test_file = 'eng.val'\n",
    "\n",
    "# Load the training data\n",
    "train_sentences = load_sentences(train_file)\n",
    "train_corpus, dic = prepare_dataset(train_sentences, mode='train', word_to_id=None, tag_to_id=None)\n",
    "\n",
    "# Load the testing data\n",
    "test_sentences = load_sentences(test_file)\n",
    "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[8154. 1349. 2455. 1581.  502.]\n"
     ]
    }
   ],
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus = train_corpus,\n",
    "    test_corpus = test_corpus,\n",
    "    dic = dic,\n",
    "    decode_type = 'greedy',\n",
    "    output_file = None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus = train_corpus,\n",
    "    test_corpus = test_corpus,\n",
    "    dic = dic,\n",
    "    decode_type = 'viterbi',\n",
    "    output_file = None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import collections\n",
    "import codecs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MEMM(object):\n",
    "    \"\"\"\n",
    "    Max-Entropy Markov Model (MEMM) for Named Entity Recognition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dic):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "        self.word_to_id = dic['word_to_id']\n",
    "        self.tag_to_id = dic['tag_to_id']\n",
    "        self.id_to_tag = dic['id_to_tag']\n",
    "        self.tag_to_id['<start>'] = len(self.tag_to_id)\n",
    "        self.id_to_tag[len(self.tag_to_id)] = '<start>'\n",
    "\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def prepare_data(self, corpus):\n",
    "        \"\"\"\n",
    "        Prepare data for training the MEMM.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence['words'])):\n",
    "                features = self.extract_features(sentence['words'], i)\n",
    "                X.append(features)\n",
    "                y.append(sentence['tags'][i])\n",
    "\n",
    "        X = self.vectorizer.fit_transform(X)\n",
    "        return X, y\n",
    "\n",
    "    def extract_features(self, words, index):\n",
    "        \"\"\"\n",
    "        Extract features for a given word in a sentence.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Current word\n",
    "        features['word'] = words[index]\n",
    "\n",
    "        # Previous tag\n",
    "        if index > 0:\n",
    "            features['prev_tag'] = self.id_to_tag[words[index - 1]]\n",
    "\n",
    "        return features\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the MEMM model.\n",
    "        \"\"\"\n",
    "        X, y = self.prepare_data(corpus)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def decode(self, sentence):\n",
    "        \"\"\"\n",
    "        Decode a single sentence using MEMM.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "\n",
    "        for i in range(len(sentence['words'])):\n",
    "            features = self.extract_features(sentence['words'], i)\n",
    "            X = self.vectorizer.transform([features])\n",
    "            tag_id = self.model.predict(X)[0]\n",
    "            tags.append(tag_id)\n",
    "\n",
    "        return tags"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def runMEMM(train_corpus, test_corpus, dic, output_file):\n",
    "    # Build and train the MEMM model\n",
    "    model = MEMM(dic)\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    # Tagging and evaluation on training data\n",
    "    print(\"\\nTrain results:\")\n",
    "    pred_train, real_train = tag_corpus(model, train_corpus, output_file, dic)\n",
    "\n",
    "    # Tagging and evaluation on test data\n",
    "    print(\"\\nTest results:\")\n",
    "    pred_test, real_test = tag_corpus(model, test_corpus, output_file, dic)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(train_file)\n",
    "train_corpus, dic = prepare_dataset(train_sentences, mode='train')\n",
    "test_sentences = load_sentences(test_file)\n",
    "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run MEMM model\n",
    "runMEMM(train_corpus, test_corpus, dic, output_file=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
