{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Problem 1** : HMM for NER"
   ],
   "metadata": {
    "id": "ZVo_RhVyUmDZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import codecs"
   ],
   "metadata": {
    "id": "XMUIflIVU5ce"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace all digits in a string with zeros.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path):\n",
    "    \"\"\"\n",
    "    Load sentences. A line must contain at least a word and its tag.\n",
    "    Sentences are separated by empty lines.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip())\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ],
   "metadata": {
    "id": "C26j60xvU-to"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico"
   ],
   "metadata": {
    "id": "Patp2CNOVJl1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items) if v[1] > 2}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words))\n",
    "    )\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ],
   "metadata": {
    "id": "7o0lJRoHVMtI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def prepare_dataset(sentences, mode=None, word_to_id=None, tag_to_id=None):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return 'data', which is a list of lists of dictionaries containing:\n",
    "        - words (strings)\n",
    "        - word indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    assert mode == 'train' or (mode == 'test' and word_to_id and tag_to_id)\n",
    "\n",
    "    if mode=='train':\n",
    "        word_dic, word_to_id, id_to_word = word_mapping(sentences)\n",
    "        tag_dic, tag_to_id, id_to_tag = tag_mapping(sentences)\n",
    "\n",
    "    def f(x): return x\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'tags': tags,\n",
    "        })\n",
    "\n",
    "    if mode == 'train':\n",
    "        return data, {'word_to_id':word_to_id, 'id_to_word':id_to_word, 'tag_to_id':tag_to_id, 'id_to_tag':id_to_tag}\n",
    "    else:\n",
    "        return data"
   ],
   "metadata": {
    "id": "ACB_BTf3VO17"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "import collections\n",
    "import codecs"
   ],
   "metadata": {
    "id": "qkq5YrkjVRcV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class HMM(object):\n",
    "    \"\"\"\n",
    "     HMM Model\n",
    "    \"\"\"\n",
    "    def __init__(self, dic, decode_type):\n",
    "        \"\"\"\n",
    "        Initialize the model.\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_words = len(dic['word_to_id'])\n",
    "        self.num_tags = len(dic['tag_to_id'])\n",
    "\n",
    "        self.initial_prob = np.ones([self.num_tags])\n",
    "        self.transition_prob = np.ones([self.num_tags, self.num_tags])\n",
    "        self.emission_prob = np.ones([self.num_tags, self.num_words])\n",
    "        self.decode_type = decode_type\n",
    "\n",
    "        return\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        TODO: Train a bigram HMM model using MLE estimates.\n",
    "        Complete the code to compute self,initial_prob, self.transition_prob and self.emission_prob appropriately.\n",
    "\n",
    "        Args:\n",
    "            corpus is a list of dictionaries of the form:\n",
    "            {'str_words': str_words,   ### List of string words\n",
    "            'words': words,            ### List of word IDs\n",
    "            'tags': tags}              ### List of tag IDs\n",
    "            All three lists above have length equal to the sentence length for each instance.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Compute initial_probs.\n",
    "        # initial_prob[x]: the probability of tag x to be assigned to the first token in a sentence.\n",
    "        self.initial_prob = np.zeros([self.num_tags])\n",
    "        print(self.initial_prob)\n",
    "        for sentence in corpus:\n",
    "            # TODO: update self.initial_prob\n",
    "            # (5 points)\n",
    "            # START HERE\n",
    "            tag = sentence['tags']\n",
    "            self.initial_prob[tag[0]] += 1\n",
    "            # END HERE\n",
    "        print(self.initial_prob)\n",
    "\n",
    "        # Normarlize initial_prob to sum to 1\n",
    "        self.initial_prob /= np.sum(self.initial_prob)\n",
    "\n",
    "        # Step 2: Complete the code to compute transition_prob.\n",
    "        # transition_prob[x][y]: the probability of tag y to appear after tag x.\n",
    "        self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
    "        for sentence in corpus:\n",
    "            tag = sentence['tags']\n",
    "            for i in range(1, len(tag)):\n",
    "                # TODO: update self.transition_prob\n",
    "                # (5 points)\n",
    "                # START HERE\n",
    "                 self.transition_prob = np.zeros([self.num_tags, self.num_tags])\n",
    "            for sentence in corpus:\n",
    "                tag = sentence['tags']\n",
    "                for i in range(1, len(tag)):\n",
    "                    self.transition_prob[tag[i-1]][tag[i]] += 1\n",
    "                # END HERE\n",
    "        print(self.transition_prob)\n",
    "\n",
    "        # Normalize every row of transition_prob to sum to 1.\n",
    "        for p in self.transition_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "\n",
    "        # Step 3: Complete the code to compute emission_prob\n",
    "        # emission_prob[x][y]: the probability of word y to appear given tag x.\n",
    "        # Note that for each sentence s in the corpus, word IDs are in s['words'].\n",
    "        self.emission_prob = np.zeros([self.num_tags, self.num_words])\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence['tags'])):\n",
    "                # TODO: update self.emission_prob\n",
    "                # (5 points)\n",
    "                # START HERE\n",
    "                tag = sentence['tags'][i]\n",
    "                word = sentence['words'][i]\n",
    "                self.emission_prob[tag][word] += 1\n",
    "                # END HERE\n",
    "        print(self.emission_prob)\n",
    "\n",
    "        # For every tag, normalize the emission_prob to sum to 1.\n",
    "        for p in self.emission_prob:\n",
    "            p /= np.sum(p)\n",
    "\n",
    "        return\n",
    "\n",
    "    def greedy_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        Decode a single sentence in Greedy fashion\n",
    "        Return a list of tags.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "\n",
    "        init_scores = [self.initial_prob[t] * self.emission_prob[t][sentence[0]] for t in range(self.num_tags)]\n",
    "        tags.append(np.argmax(init_scores))\n",
    "\n",
    "        for w in sentence[1:]:\n",
    "            scores = [self.transition_prob[tags[-1]][t] * self.emission_prob[t][w] for t in range(self.num_tags)]\n",
    "            tags.append(np.argmax(scores))\n",
    "\n",
    "        assert len(tags) == len(sentence)\n",
    "        return tags\n",
    "\n",
    "    def viterbi_decode(self, sentence):\n",
    "        \"\"\"\n",
    "        TODO: Complete the code to decode a single sentence using the Viterbi algorithm.\n",
    "        Args:\n",
    "             sentence -- a list of ints that represents word IDs.\n",
    "        Output:\n",
    "             tags     -- a list of ints that represents the tags decoded from the input.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "        l = len(sentence)\n",
    "\n",
    "        pi = self.initial_prob\n",
    "        A = self.transition_prob\n",
    "        O = self.emission_prob\n",
    "\n",
    "        # Let M be the state matrix.\n",
    "        # M[i,j]: most probable sequence of tags ending with tag j at the i-th token.\n",
    "        M = np.zeros((l, self.num_tags))\n",
    "        M[:,:] = float('-inf')\n",
    "\n",
    "        # Use B to track the path to reach the most probable sequence.\n",
    "        # B[i,j] is the tag of the (i-1)-th token in the most probable sequence ending with tag j at the i-th token.\n",
    "        B = np.zeros((l, self.num_tags), 'int')\n",
    "\n",
    "        # Compute the probability to assign each tag to the first token.\n",
    "        M[0, :] = pi * O[:, sentence[0]]\n",
    "\n",
    "        # Dynamic programming.\n",
    "        for i in range(1, l):\n",
    "            for j in range(self.num_tags):\n",
    "                # TODO: Compute M[i, j] and B[i, j].\n",
    "                # (10 points)\n",
    "                # START HERE\n",
    "                max_prob = float('-inf')\n",
    "                max_tag = None\n",
    "                for k in range(self.num_tags):\n",
    "                  prob = M[i-1, k] * A[k, j] * O[j, sentence[i]]\n",
    "                  if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_tag = k\n",
    "                M[i, j] = max_prob\n",
    "                B[i, j] = max_tag\n",
    "                # END HERE\n",
    "\n",
    "        # Extract the optimal sequence of tags from B.\n",
    "        # Start from the last position, and iteratively find the tag of each position that results in the most probable tag sequence.\n",
    "        tags.append(np.argmax(M[l-1,:]))\n",
    "        for i in range(l-1, 0, -1):\n",
    "            # TODO: Extract the tag of the (i-1)-th token that results in the most probable sequence of tags.\n",
    "            # (5 points)\n",
    "            # START HERE\n",
    "            tags.append(B[i, tags[-1]])\n",
    "            # END HERE\n",
    "\n",
    "        return tags\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        \"\"\"\n",
    "        Tag a sentence using a trained HMM.\n",
    "        \"\"\"\n",
    "        if self.decode_type == 'viterbi':\n",
    "            return self.viterbi_decode(sentence)\n",
    "        elif self.decode_type == 'greedy':\n",
    "            return self.greedy_decode(sentence)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown decoding type\")"
   ],
   "metadata": {
    "id": "DKbF0lbpVSRT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn.metrics import f1_score, confusion_matrix"
   ],
   "metadata": {
    "id": "r35vkYb6VVXB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tag_corpus(model, test_corpus, output_file, dic):\n",
    "    if output_file:\n",
    "        f_output = codecs.open(output_file, 'w', 'utf-8')\n",
    "    start = time.time()\n",
    "\n",
    "    num_correct = 0.\n",
    "    num_total = 0.\n",
    "    y_pred=[]\n",
    "    y_actual=[]\n",
    "    print('Tagging...')\n",
    "    for i, sentence in enumerate(tqdm(test_corpus)):\n",
    "        tags = model.tag(sentence['words'])\n",
    "        str_tags = [dic['id_to_tag'][t] for t in tags]\n",
    "        y_pred.extend(tags)\n",
    "        y_actual.extend(sentence['tags'])\n",
    "\n",
    "        # Check accuracy.\n",
    "        num_correct += np.sum(np.array(tags) == np.array(sentence['tags']))\n",
    "        num_total += len([w for w in sentence['words']])\n",
    "\n",
    "        if output_file:\n",
    "            f_output.write('%s\\n' % ' '.join('%s%s%s' % (w, '__', y)\n",
    "                                             for w, y in zip(sentence['str_words'], str_tags)))\n",
    "\n",
    "    print('---- %i lines tagged in %.4fs ----' % (len(test_corpus), time.time() - start))\n",
    "    if output_file:\n",
    "        f_output.close()\n",
    "\n",
    "    print(\"Overall accuracy: %s\\n\" % (num_correct/num_total))\n",
    "    return y_pred,y_actual"
   ],
   "metadata": {
    "id": "_nZOET9LVXrb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_score(y_pred,y_actual):\n",
    "    A = confusion_matrix(y_actual, y_pred)\n",
    "    f1 = f1_score(y_actual, y_pred,average=None)\n",
    "    print(\"Confusion Matrix:\\n\", A)\n",
    "    print(\"F-1 scores: \", f1)"
   ],
   "metadata": {
    "id": "eyrCVDUEVZ5Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def runHiddenMarkovModel(train_corpus,\n",
    "                         test_corpus,\n",
    "                         dic,\n",
    "                         decode_type,\n",
    "                         output_file):\n",
    "    # build and train the model\n",
    "    model = HMM(dic, decode_type)\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    print(\"Train results:\")\n",
    "    pred, real = tag_corpus(model, train_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic['id_to_tag'])\n",
    "    A = compute_score(pred,real)\n",
    "\n",
    "    # test on validation\n",
    "    print(\"\\n-----------\\nTest results:\")\n",
    "    pred, real = tag_corpus(model, test_corpus, output_file, dic)\n",
    "\n",
    "    print(\"Tags: \", dic['id_to_tag'])\n",
    "    A = compute_score(pred,real)"
   ],
   "metadata": {
    "id": "zdW0Zw9aVbeE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Download the dataset\n",
    "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
    "!wget https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
    "\n",
    "train_file = 'eng.train'\n",
    "test_file = 'eng.val'\n",
    "\n",
    "# Load the training data\n",
    "train_sentences = load_sentences(train_file)\n",
    "train_corpus, dic = prepare_dataset(train_sentences, mode='train', word_to_id=None, tag_to_id=None)\n",
    "\n",
    "# Load the testing data\n",
    "test_sentences = load_sentences(test_file)\n",
    "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])"
   ],
   "metadata": {
    "id": "_HyntzHeVefj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97d76554-2466-41bd-ebb5-f1b2dc1d0632"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2024-03-22 20:23:34--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
      "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3213441 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘eng.train.1’\n",
      "\n",
      "eng.train.1         100%[===================>]   3.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-03-22 20:23:34 (75.8 MB/s) - ‘eng.train.1’ saved [3213441/3213441]\n",
      "\n",
      "--2024-03-22 20:23:34--  https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
      "Resolving princeton-nlp.github.io (princeton-nlp.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
      "Connecting to princeton-nlp.github.io (princeton-nlp.github.io)|185.199.108.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 774436 (756K) [application/octet-stream]\n",
      "Saving to: ‘eng.val.1’\n",
      "\n",
      "eng.val.1           100%[===================>] 756.29K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2024-03-22 20:23:35 (21.7 MB/s) - ‘eng.val.1’ saved [774436/774436]\n",
      "\n",
      "Found 20101 unique words (203621 in total)\n",
      "Found 5 unique named entity tags\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus = train_corpus,\n",
    "    test_corpus = test_corpus,\n",
    "    dic = dic,\n",
    "    decode_type = 'greedy',\n",
    "    output_file = None\n",
    ")"
   ],
   "metadata": {
    "id": "KFBuIbUbVgqG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a83049c4-1070-4ae4-9887-1163d022a5d8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[8154. 1349. 2455. 1581.  502.]\n",
      "[[1.38886e+05 5.18300e+03 3.79200e+03 5.53300e+03 2.86100e+03]\n",
      " [6.35400e+03 4.52800e+03 0.00000e+00 3.00000e+00 1.00000e+00]\n",
      " [6.10500e+03 6.00000e+00 3.72800e+03 2.00000e+00 9.00000e+00]\n",
      " [6.92700e+03 1.00000e+00 1.10000e+01 1.16800e+03 3.00000e+01]\n",
      " [3.15200e+03 6.10000e+01 3.90000e+01 1.00000e+01 1.19000e+03]]\n",
      "[[9.112e+03 7.362e+03 7.275e+03 ... 3.000e+00 3.000e+00 3.000e+00]\n",
      " [3.394e+03 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.842e+03 6.000e+00 1.300e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [9.150e+02 4.000e+00 1.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [5.510e+02 2.000e+00 1.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "Train results:\n",
      "Tagging...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14041/14041 [00:02<00:00, 6246.33it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- 14041 lines tagged in 2.2602s ----\n",
      "Overall accuracy: 0.9544742438157164\n",
      "\n",
      "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
      "Confusion Matrix:\n",
      " [[168060    135   1233     48    102]\n",
      " [  1999   8628    456     39      6]\n",
      " [  1632     98   7291    731    273]\n",
      " [   741     49    567   6886     54]\n",
      " [   665     32    206    204   3486]]\n",
      "F-1 scores:  [0.98087109 0.85979073 0.73728385 0.84986115 0.81888654]\n",
      "\n",
      "-----------\n",
      "Test results:\n",
      "Tagging...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3490/3490 [00:00<00:00, 6639.36it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- 3490 lines tagged in 0.5346s ----\n",
      "Overall accuracy: 0.9241331540561464\n",
      "\n",
      "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
      "Confusion Matrix:\n",
      " [[40558    17   543    17    29]\n",
      " [ 1105  1294   267    16     8]\n",
      " [  606    30  1382   176    56]\n",
      " [  249    15   215  1478    18]\n",
      " [  233     8    79    37   650]]\n",
      "F-1 scores:  [0.96664482 0.63838185 0.58361486 0.7991349  0.73529412]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "runHiddenMarkovModel(\n",
    "    train_corpus = train_corpus,\n",
    "    test_corpus = test_corpus,\n",
    "    dic = dic,\n",
    "    decode_type = 'viterbi',\n",
    "    output_file = None\n",
    ")"
   ],
   "metadata": {
    "id": "xam_wzAoViw8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b9d2ea25-fce6-4242-dc3f-473007c56e9b"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0. 0. 0. 0. 0.]\n",
      "[8154. 1349. 2455. 1581.  502.]\n",
      "[[1.38886e+05 5.18300e+03 3.79200e+03 5.53300e+03 2.86100e+03]\n",
      " [6.35400e+03 4.52800e+03 0.00000e+00 3.00000e+00 1.00000e+00]\n",
      " [6.10500e+03 6.00000e+00 3.72800e+03 2.00000e+00 9.00000e+00]\n",
      " [6.92700e+03 1.00000e+00 1.10000e+01 1.16800e+03 3.00000e+01]\n",
      " [3.15200e+03 6.10000e+01 3.90000e+01 1.00000e+01 1.19000e+03]]\n",
      "[[9.112e+03 7.362e+03 7.275e+03 ... 3.000e+00 3.000e+00 3.000e+00]\n",
      " [3.394e+03 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [1.842e+03 6.000e+00 1.300e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [9.150e+02 4.000e+00 1.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [5.510e+02 2.000e+00 1.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "Train results:\n",
      "Tagging...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14041/14041 [00:04<00:00, 2855.30it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- 14041 lines tagged in 4.9247s ----\n",
      "Overall accuracy: 0.7389807534586315\n",
      "\n",
      "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
      "Confusion Matrix:\n",
      " [[146237   7558   7024   5561   3198]\n",
      " [  8217   1638    333    792    148]\n",
      " [  8024    400   1309    222     70]\n",
      " [  6460    662    297    741    137]\n",
      " [  3693    134     78    141    547]]\n",
      "F-1 scores:  [0.85466484 0.15223048 0.13731249 0.09407135 0.12584838]\n",
      "\n",
      "-----------\n",
      "Test results:\n",
      "Tagging...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3490/3490 [00:01<00:00, 2954.68it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- 3490 lines tagged in 1.1879s ----\n",
      "Overall accuracy: 0.7566923359002566\n",
      "\n",
      "Tags:  {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
      "Confusion Matrix:\n",
      " [[36446  1511  1421  1134   652]\n",
      " [ 2020   267    46   333    24]\n",
      " [ 1918    61   212    45    14]\n",
      " [ 1587   189    48   123    28]\n",
      " [  822    31    22    37    95]]\n",
      "F-1 scores:  [0.86820634 0.11244473 0.10602651 0.0674527  0.1043956 ]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **Problem** : MEMM for NER"
   ],
   "metadata": {
    "id": "kXJuwhPMQ9VX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import collections\n",
    "import codecs"
   ],
   "metadata": {
    "id": "H4v7eNdIrRGk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MEMM(object):\n",
    "    \"\"\"\n",
    "    Max-Entropy Markov Model (MEMM) for Named Entity Recognition.\n",
    "    \"\"\"\n",
    "    def __init__(self, dic):\n",
    "        self.word_to_id = dic['word_to_id']\n",
    "\n",
    "        # Initialize tag_to_id and id_to_tag with consecutive integer IDs\n",
    "        self.tag_to_id = {tag: i for i, tag in enumerate(dic['tag_to_id'])}\n",
    "        self.id_to_tag = {i: tag for tag, i in self.tag_to_id.items()}\n",
    "\n",
    "        # Add '<start>' tag and its corresponding ID\n",
    "        self.tag_to_id['<start>'] = len(self.tag_to_id)\n",
    "        self.id_to_tag[len(self.tag_to_id)] = '<start>'\n",
    "\n",
    "        # Initialize DictVectorizer and LogisticRegression model\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def prepare_data(self, corpus):\n",
    "        \"\"\"\n",
    "        Prepare data for training the MEMM.\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        for sentence in corpus:\n",
    "            for i in range(len(sentence['words'])):\n",
    "                features = self.extract_features(sentence['words'], i)\n",
    "                X.append(features)\n",
    "                y.append(sentence['tags'][i])\n",
    "\n",
    "        X = self.vectorizer.fit_transform(X)\n",
    "        return X, y\n",
    "\n",
    "    def extract_features(self, words, index):\n",
    "        \"\"\"\n",
    "        Extract features for a given word in a sentence.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Previous tag\n",
    "        if index > 0:\n",
    "            features['prev_tag'] = self.id_to_tag.get(words[index - 1])\n",
    "        return features\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the MEMM model.\n",
    "        \"\"\"\n",
    "        X, y = self.prepare_data(corpus)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def decode(self, sentence):\n",
    "        \"\"\"\n",
    "        Decode a single sentence using MEMM.\n",
    "        \"\"\"\n",
    "        tags = []\n",
    "\n",
    "        for i in range(len(sentence['words'])):\n",
    "            features = self.extract_features(sentence['words'], i)\n",
    "            X = self.vectorizer.transform([features])\n",
    "            tag_id = self.model.predict(X)[0]\n",
    "            tags.append(tag_id)\n",
    "\n",
    "        return tags"
   ],
   "metadata": {
    "id": "zNKh20cOkcG9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Load and preprocess the data\n",
    "train_sentences = load_sentences(train_file)\n",
    "train_corpus, dic = prepare_dataset(train_sentences, mode='train')  # Specify mode='train'\n",
    "\n",
    "test_sentences = load_sentences(test_file)\n",
    "test_corpus = prepare_dataset(test_sentences, mode='test', word_to_id=dic['word_to_id'], tag_to_id=dic['tag_to_id'])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d3G3Ezqz2Wm",
    "outputId": "41fb063e-d997-4a28-9ad3-ac35c9949719"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 20101 unique words (203621 in total)\n",
      "Found 5 unique named entity tags\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def runMEMM(train_corpus, test_corpus, dic, output_file):\n",
    "    # Build and train the MEMM model\n",
    "    model = MEMM(dic)\n",
    "    model.train(train_corpus)\n",
    "\n",
    "    # Tagging and evaluation on training data\n",
    "    print(\"\\nTrain results:\")\n",
    "    pred_train, real_train = tag_corpus(model, train_corpus, output_file, dic)\n",
    "\n",
    "    # Tagging and evaluation on test data\n",
    "    print(\"\\nTest results:\")\n",
    "    pred_test, real_test = tag_corpus(model, test_corpus, output_file, dic)"
   ],
   "metadata": {
    "id": "blAivjbHkeul"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run MEMM model\n",
    "runMEMM(train_corpus, test_corpus, dic, output_file=None)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-HtIoGYkiwe",
    "outputId": "aaeeb36c-14d1-4282-da51-85c65b46356e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-59-4e0cf00797a3>\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Run MEMM model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mrunMEMM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_corpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_corpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdic\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_file\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-58-3fb1f97a20fd>\u001B[0m in \u001B[0;36mrunMEMM\u001B[0;34m(train_corpus, test_corpus, dic, output_file)\u001B[0m\n\u001B[1;32m      2\u001B[0m     \u001B[0;31m# Build and train the MEMM model\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMEMM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdic\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_corpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;31m# Tagging and evaluation on training data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-56-bc8fbd77d973>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, corpus)\u001B[0m\n\u001B[1;32m     49\u001B[0m         \"\"\"\n\u001B[1;32m     50\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprepare_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcorpus\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdecode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msentence\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m   1194\u001B[0m             \u001B[0m_dtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1195\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1196\u001B[0;31m         X, y = self._validate_data(\n\u001B[0m\u001B[1;32m   1197\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1198\u001B[0m             \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    582\u001B[0m                 \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"y\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_y_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    583\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 584\u001B[0;31m                 \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcheck_X_y\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    585\u001B[0m             \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    586\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_X_y\u001B[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[1;32m   1104\u001B[0m         )\n\u001B[1;32m   1105\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1106\u001B[0;31m     X = check_array(\n\u001B[0m\u001B[1;32m   1107\u001B[0m         \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1108\u001B[0m         \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    843\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0msp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0missparse\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    844\u001B[0m         \u001B[0m_ensure_no_complex_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 845\u001B[0;31m         array = _ensure_sparse_format(\n\u001B[0m\u001B[1;32m    846\u001B[0m             \u001B[0marray\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    847\u001B[0m             \u001B[0maccept_sparse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maccept_sparse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m_ensure_sparse_format\u001B[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    559\u001B[0m             )\n\u001B[1;32m    560\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 561\u001B[0;31m             _assert_all_finite(\n\u001B[0m\u001B[1;32m    562\u001B[0m                 \u001B[0mspmatrix\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    563\u001B[0m                 \u001B[0mallow_nan\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mforce_all_finite\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"allow-nan\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001B[0m in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;34m\"#estimators-that-handle-nan-values\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             )\n\u001B[0;32m--> 161\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmsg_err\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ]
  }
 ]
}
